# -*- coding: utf-8 -*-
"""World_Happiness_report_Prediction_augmented_pol_classification_statistique_analyse

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SEuwJQDx_EXiYtZC_QXUEeliunL77trV
"""

import pandas as pd
df = pd.read_csv("/content/df_vf_1.csv")
df.head()
df = df.dropna()
df.isnull().sum()

df["Orientation politique"].unique()

df.astype(numerical)

test = df["Life Ladder"] == "Centre"
test.value_counts()
df = df[df['Life Ladder'] != 'Centre']

df["Life Ladder"] = df["Life Ladder"].astype(float)

df_clean = df[["Life Ladder", "Orientation politique"]]

df["Orientation politique"].unique()



"""Visualisation des distributions pour voir si elles sont normales afin d'utiliser le test statistique de Anova"""

import matplotlib.pyplot as plt
import seaborn as sns

centre_data = df[df['Orientation politique'] == 'Centre']
Autoritaire_data = df[df['Orientation politique'] == 'Autoritaire']
Extrême_droite_data = df[df['Orientation politique'] == 'Extrême-droite']
Gauche_data = df[df['Orientation politique'] == 'Gauche']
Droite_data = df[df['Orientation politique'] == 'Droite']
Extrême_gauche_data = df[df['Orientation politique'] == 'Extrême-gauche']

# Histogramme
plt.figure(figsize=(8, 6))
sns.histplot(centre_data['Life Ladder'], bins=20)  # Ajuster le nombre de bins si nécessaire
plt.title('Distribution de Life Ladder pour la catégorie Centre')
plt.xlabel('Life Ladder')
plt.ylabel('Fréquence')
plt.show()
sns.histplot(Autoritaire_data['Life Ladder'], bins=20)  # Ajuster le nombre de bins si nécessaire
plt.title('Distribution de Life Ladder pour la catégorie Centre')
plt.xlabel('Life Ladder')
plt.ylabel('Fréquence')
plt.show()
sns.histplot(Extrême_droite_data['Life Ladder'], bins=20)  # Ajuster le nombre de bins si nécessaire
plt.title('Distribution de Life Ladder pour la catégorie Centre')
plt.xlabel('Life Ladder')
plt.ylabel('Fréquence')
plt.show()
sns.histplot(Gauche_data['Life Ladder'], bins=20)  # Ajuster le nombre de bins si nécessaire
plt.title('Distribution de Life Ladder pour la catégorie Centre')
plt.xlabel('Life Ladder')
plt.ylabel('Fréquence')
plt.show()
sns.histplot(Droite_data['Life Ladder'], bins=20)  # Ajuster le nombre de bins si nécessaire
plt.title('Distribution de Life Ladder pour la catégorie Centre')
plt.xlabel('Life Ladder')
plt.ylabel('Fréquence')
plt.show()
sns.histplot(Extrême_gauche_data['Life Ladder'], bins=20)  # Ajuster le nombre de bins si nécessaire
plt.title('Distribution de Life Ladder pour la catégorie Centre')
plt.xlabel('Life Ladder')
plt.ylabel('Fréquence')
plt.show()



"""Test de Shapiro pour vérifier mathématiquement en computation que nos distribution sont anormales"""

df["Orientation politique"].unique()

import numpy as np
from scipy.stats import shapiro
shapiro(centre_data["Life Ladder"])

shapiro(Autoritaire_data["Life Ladder"])

shapiro(Extrême_droite_data["Life Ladder"])

shapiro(Gauche_data["Life Ladder"])

shapiro(Droite_data["Life Ladder"])

shapiro(Extrême_gauche_data["Life Ladder"])

"""Test de Kruscal pour déterminer si il y a une différence entre au moins deux groupe. Le test rejette l'hypothèse null --> Pas de différence"""

import seaborn as sns
from scipy import stats

# Faire le test

kstat, pval = stats.kruskal(*[group["Life Ladder"].values for name, group in df.groupby("Orientation politique")])

# Récupérer la p-value

print(pval) # Si pval petit (au moins inférieure à 0.05, alors on peut parler de différence significative)

! pip install scikit_posthocs

"""Pour montrer quels sont les différences majeures entre les groupes"""

import scikit_posthocs as sp
import pandas as pd
# Example DataFrame
data = pd.DataFrame({
    'wait_time': [2, 3, 5, 8, 1, 10, 6, 7, 9, 4, 12],
    'review_score': [5, 4, 3, 5, 2, 1, 4, 3, 2, 5, 1]
})
# Dunn's Test
dunn_result = sp.posthoc_dunn(
    df,
    val_col='Life Ladder',  # Column containing the variable of interest
    group_col='Orientation politique',  # Column containing the group labels
    p_adjust='bonferroni'  # Correct for multiple comparisons
)
print(dunn_result)

for cat in df["Orientation politique"].unique():
  print(cat)
  print(df[df["Orientation politique"] == cat]["Life Ladder"].median())
  print("\n")

centre_data = df[df['Orientation politique'] == 'Centre']
Autoritaire_data = df[df['Orientation politique'] == 'Autoritaire']
Extrême_droite_data = df[df['Orientation politique'] == 'Extrême-droite']
Gauche_data = df[df['Orientation politique'] == 'Gauche']
Droite_data = df[df['Orientation politique'] == 'Droite']
Extrême_gauche_data = df[df['Orientation politique'] == 'Extrême-gauche']



centre_data.columns

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
X = centre_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique'])
y = centre_data['Life Ladder']

X_train, X_test, y_train, y_test = train_test_split(X, y)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

clf = rf_model.fit(X_train, y_train)

importance = clf.feature_importances_

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained your Random Forest model (rf_model) and calculated feature importances (importance)

# Get feature names from df_en_dev
feature_names = centre_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique']).columns

# Sort features by importance in descending order
sorted_indices = np.argsort(importance)[::-1]  # Get indices of sorted importances (descending)
sorted_features = feature_names[sorted_indices]
sorted_importance = importance[sorted_indices]

# Create the horizontal bar plot
plt.barh(range(len(sorted_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_features)), sorted_features)  # Label features on the y-axis
plt.gca().invert_yaxis()  # Invert y-axis for descending order
plt.xlabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
X = Autoritaire_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique'])
y = Autoritaire_data['Life Ladder']

X_train, X_test, y_train, y_test = train_test_split(X, y)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

clf = rf_model.fit(X_train, y_train)

importance = clf.feature_importances_

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained your Random Forest model (rf_model) and calculated feature importances (importance)

# Get feature names from df_en_dev
feature_names = Autoritaire_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique']).columns

# Sort features by importance in descending order
sorted_indices = np.argsort(importance)[::-1]  # Get indices of sorted importances (descending)
sorted_features = feature_names[sorted_indices]
sorted_importance = importance[sorted_indices]

# Create the horizontal bar plot
plt.barh(range(len(sorted_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_features)), sorted_features)  # Label features on the y-axis
plt.gca().invert_yaxis()  # Invert y-axis for descending order
plt.xlabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
X = Extrême_droite_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique'])
y = Extrême_droite_data['Life Ladder']

X_train, X_test, y_train, y_test = train_test_split(X, y)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

clf = rf_model.fit(X_train, y_train)

importance = clf.feature_importances_

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained your Random Forest model (rf_model) and calculated feature importances (importance)

# Get feature names from df_en_dev
feature_names = Extrême_droite_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique']).columns

# Sort features by importance in descending order
sorted_indices = np.argsort(importance)[::-1]  # Get indices of sorted importances (descending)
sorted_features = feature_names[sorted_indices]
sorted_importance = importance[sorted_indices]

# Create the horizontal bar plot
plt.barh(range(len(sorted_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_features)), sorted_features)  # Label features on the y-axis
plt.gca().invert_yaxis()  # Invert y-axis for descending order
plt.xlabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
X = Gauche_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique'])
y = Gauche_data['Life Ladder']

X_train, X_test, y_train, y_test = train_test_split(X, y)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

clf = rf_model.fit(X_train, y_train)

importance = clf.feature_importances_

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained your Random Forest model (rf_model) and calculated feature importances (importance)

# Get feature names from df_en_dev
feature_names = Gauche_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique']).columns

# Sort features by importance in descending order
sorted_indices = np.argsort(importance)[::-1]  # Get indices of sorted importances (descending)
sorted_features = feature_names[sorted_indices]
sorted_importance = importance[sorted_indices]

# Create the horizontal bar plot
plt.barh(range(len(sorted_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_features)), sorted_features)  # Label features on the y-axis
plt.gca().invert_yaxis()  # Invert y-axis for descending order
plt.xlabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
X = Droite_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique'])
y = Droite_data['Life Ladder']

X_train, X_test, y_train, y_test = train_test_split(X, y)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

clf = rf_model.fit(X_train, y_train)

importance = clf.feature_importances_

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained your Random Forest model (rf_model) and calculated feature importances (importance)

# Get feature names from df_en_dev
feature_names = Droite_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique']).columns

# Sort features by importance in descending order
sorted_indices = np.argsort(importance)[::-1]  # Get indices of sorted importances (descending)
sorted_features = feature_names[sorted_indices]
sorted_importance = importance[sorted_indices]

# Create the horizontal bar plot
plt.barh(range(len(sorted_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_features)), sorted_features)  # Label features on the y-axis
plt.gca().invert_yaxis()  # Invert y-axis for descending order
plt.xlabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
X = Extrême_gauche_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique'])
y = Extrême_gauche_data['Life Ladder']

X_train, X_test, y_train, y_test = train_test_split(X, y)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

clf = rf_model.fit(X_train, y_train)

importance = clf.feature_importances_

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have already trained your Random Forest model (rf_model) and calculated feature importances (importance)

# Get feature names from df_en_dev
feature_names = Extrême_gauche_data.drop(columns=['Life Ladder', 'Country name','year','region','sub-region','Category','clef', 'Pays', 'Prénom',
       'Nom de famille', 'Rôle', 'Régime politique', 'Orientation politique']).columns

# Sort features by importance in descending order
sorted_indices = np.argsort(importance)[::-1]  # Get indices of sorted importances (descending)
sorted_features = feature_names[sorted_indices]
sorted_importance = importance[sorted_indices]

# Create the horizontal bar plot
plt.barh(range(len(sorted_features)), sorted_importance, align='center')
plt.yticks(range(len(sorted_features)), sorted_features)  # Label features on the y-axis
plt.gca().invert_yaxis()  # Invert y-axis for descending order
plt.xlabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.show()

"""Conclusion certaine catégorie peuvent-être fusionnée pour obtenir un meilleur résultat avec notre modèle"""

import matplotlib.pyplot as plt
import numpy as np

# Fonction pour afficher les graphiques d'importance
def plot_feature_importance(ax, features, importance, title):
    sorted_indices = np.argsort(importance)[::-1]
    sorted_features = np.array(features)[sorted_indices]
    sorted_importance = np.array(importance)[sorted_indices]
    ax.barh(range(len(sorted_features)), sorted_importance, align='center')
    ax.set_yticks(range(len(sorted_features)))
    ax.set_yticklabels(sorted_features)
    ax.invert_yaxis()  # Inverse pour un affichage décroissant
    ax.set_xlabel("Importance")
    ax.set_title(title)

# Données spécifiques à chaque modèle
categories = [
    "Centre",
    "Autoritaire",
    "Extrême droite",
    "Gauche",
    "Droite",
    "Extrême gauche",
]

# Extraites des cellules spécifiques
datasets = [
    centre_data,
    Autoritaire_data,
    Extrême_droite_data,
    Gauche_data,
    Droite_data,
    Extrême_gauche_data,
]

features_list = []
importances_list = []

# Extraction des caractéristiques et importances pour chaque modèle
for data in datasets:
    X = data.drop(
        columns=[
            "Life Ladder",
            "Country name",
            "year",
            "region",
            "sub-region",
            "Category",
            "clef",
            "Pays",
            "Prénom",
            "Nom de famille",
            "Rôle",
            "Régime politique",
            "Orientation politique",
        ]
    )
    y = data["Life Ladder"]

    # Création et ajustement du modèle
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(X, y)
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    clf = rf_model.fit(X_train, y_train)

    # Stockage des caractéristiques et importances
    features_list.append(X.columns)
    importances_list.append(clf.feature_importances_)

# Création de la figure
fig, axes = plt.subplots(3, 2, figsize=(15, 20))  # 3 lignes, 2 colonnes

for ax, category, features, importance in zip(axes.flatten(), categories, features_list, importances_list):
    plot_feature_importance(ax, features, importance, f"Feature Importance - {category}")

plt.tight_layout()
plt.show()

# Importation des bibliothèques nécessaires
import pandas as pd
from prophet import Prophet
import matplotlib.pyplot as plt

# Préparation des données
# Filtrer les pays autoritaires et autres orientations politiques
autoritaire_data = df[df['Orientation politique'] == 'Autoritaire']
other_orientations_data = df[df['Orientation politique'] != 'Autoritaire']

# Fonction pour préparer les données pour Prophet
def prepare_data_for_prophet(data, column_date='year', column_value='Life Ladder'):
    # Renommer les colonnes pour Prophet
    df_prophet = data[[column_date, column_value]].rename(columns={column_date: 'ds', column_value: 'y'})
    return df_prophet

# Préparer les données pour Prophet
autoritaire_prophet = prepare_data_for_prophet(autoritaire_data)
other_orientations_prophet = prepare_data_for_prophet(other_orientations_data)

# Définir le modèle Prophet
def fit_and_forecast(data, periods=3):
    model = Prophet()
    model.fit(data)
    future = model.make_future_dataframe(periods=periods, freq='Y')
    forecast = model.predict(future)
    return model, forecast

# Appliquer Prophet pour les deux groupes
model_autoritaire, forecast_autoritaire = fit_and_forecast(autoritaire_prophet)
model_other, forecast_other = fit_and_forecast(other_orientations_prophet)

# Visualisation des prévisions
def plot_forecast(model, forecast, title):
    model.plot(forecast)
    plt.title(title)
    plt.show()

# Afficher les prévisions pour chaque groupe
plot_forecast(model_autoritaire, forecast_autoritaire, "Prévision - Pays autoritaires")
plot_forecast(model_other, forecast_other, "Prévision - Autres orientations politiques")

# Comparaison des tendances
plt.figure(figsize=(10, 6))
plt.plot(forecast_autoritaire['ds'], forecast_autoritaire['yhat'], label='Autoritaire', linestyle='--')
plt.plot(forecast_other['ds'], forecast_other['yhat'], label='Autres orientations', linestyle='-')
plt.fill_between(forecast_autoritaire['ds'], forecast_autoritaire['yhat_lower'], forecast_autoritaire['yhat_upper'], alpha=0.2)
plt.fill_between(forecast_other['ds'], forecast_other['yhat_lower'], forecast_other['yhat_upper'], alpha=0.2)
plt.title('Comparaison des prévisions du "Life Ladder"')
plt.xlabel('Année')
plt.ylabel('Life Ladder')
plt.legend()
plt.show()

# Importation des bibliothèques nécessaires
import pandas as pd
from prophet import Prophet
import matplotlib.pyplot as plt

# Filtrer les pays autoritaires et autres orientations politiques
autoritaire_data = df[df['Orientation politique'] == 'Autoritaire']
other_orientations_data = df[df['Orientation politique'] != 'Autoritaire']

# Fonction pour préparer les données pour Prophet
def prepare_data_for_prophet(data, column_date='year', column_value='Life Ladder'):
    # Renommer les colonnes pour Prophet
    data['ds'] = pd.to_datetime(data[column_date], format='%Y')  # Conversion en datetime
    data['y'] = data[column_value]
    return data[['ds', 'y']]

# Préparer les données pour Prophet
autoritaire_prophet = prepare_data_for_prophet(autoritaire_data)
other_orientations_prophet = prepare_data_for_prophet(other_orientations_data)

# Définir le modèle Prophet
def fit_and_forecast(data, future_years=2027):
    model = Prophet()
    model.fit(data)
    # Calculer le nombre de périodes jusqu'en 2027
    last_year = data['ds'].dt.year.max()
    periods = future_years - last_year
    future = model.make_future_dataframe(periods=periods, freq='Y')
    forecast = model.predict(future)
    return model, forecast

# Appliquer Prophet pour les deux groupes
model_autoritaire, forecast_autoritaire = fit_and_forecast(autoritaire_prophet)
model_other, forecast_other = fit_and_forecast(other_orientations_prophet)

# Visualisation des prévisions
def plot_forecast(model, forecast, title):
    model.plot(forecast)
    plt.title(title)
    plt.show()

# Afficher les prévisions pour chaque groupe
plot_forecast(model_autoritaire, forecast_autoritaire, "Prévision jusqu'en 2027 - Pays autoritaires")
plot_forecast(model_other, forecast_other, "Prévision jusqu'en 2027 - Autres orientations politiques")

# Comparaison des tendances
plt.figure(figsize=(12, 6))
plt.plot(forecast_autoritaire['ds'], forecast_autoritaire['yhat'], label='Autoritaire', linestyle='--', color='blue')
plt.plot(forecast_other['ds'], forecast_other['yhat'], label='Autres orientations', linestyle='-', color='green')
plt.fill_between(forecast_autoritaire['ds'], forecast_autoritaire['yhat_lower'], forecast_autoritaire['yhat_upper'], alpha=0.2, color='blue')
plt.fill_between(forecast_other['ds'], forecast_other['yhat_lower'], forecast_other['yhat_upper'], alpha=0.2, color='green')
plt.title('Comparaison des prévisions du "Life Ladder" jusqu\'en 2027')
plt.xlabel('Année')
plt.ylabel('Life Ladder')
ax.set_facecolor("#FFE1C9")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Tracer les courbes
plt.plot(forecast_autoritaire['ds'], forecast_autoritaire['yhat'], label='Autoritaire', linestyle='--', color='blue')
plt.plot(forecast_other['ds'], forecast_other['yhat'], label='Autres orientations', linestyle='-', color='green')

# Remplir les intervalles de prévision
plt.fill_between(forecast_autoritaire['ds'], forecast_autoritaire['yhat_lower'], forecast_autoritaire['yhat_upper'], alpha=0.2, color='blue')
plt.fill_between(forecast_other['ds'], forecast_other['yhat_lower'], forecast_other['yhat_upper'], alpha=0.2, color='green')

# Définir le titre et les étiquettes
plt.title('Comparaison des prévisions du "Life Ladder" jusqu\'en 2027')
plt.xlabel('Année')
plt.ylabel('Life Ladder')

# Définir la couleur de fond
plt.gca().set_facecolor("#FFE1C9")  # gca() récupère l'axe courant

# Ajouter la légende et une grille
plt.legend()
plt.grid(True)

# Afficher le graphique
plt.show()

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Données spécifiques à chaque modèle
categories = [
    "Centre",
    "Autoritaire",
    "Extrême droite",
    "Gauche",
    "Droite",
    "Extrême gauche",
]

# Extraites des cellules spécifiques (à adapter pour fournir les vraies données)
datasets = [
    centre_data,
    Autoritaire_data,
    Extrême_droite_data,
    Gauche_data,
    Droite_data,
    Extrême_gauche_data,
]

# Liste pour stocker les résultats
results = []

for category, data in zip(categories, datasets):
    X = data.drop(
        columns=[
            "Life Ladder",
            "Country name",
            "year",
            "region",
            "sub-region",
            "Category",
            "clef",
            "Pays",
            "Prénom",
            "Nom de famille",
            "Rôle",
            "Régime politique",
            "Orientation politique",
        ]
    )
    y = data["Life Ladder"]

    # Création et ajustement du modèle
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)

    # Récupération des caractéristiques et importances
    features = X.columns
    importances = rf_model.feature_importances_

    # Ajout des résultats à la liste
    for feature, importance in zip(features, importances):
        results.append({
            "Category": category,
            "Feature": feature,
            "Importance": importance
        })

# Conversion en DataFrame
results_df = pd.DataFrame(results)

# Exportation en CSV
results_df.to_csv("feature_importances.csv", index=False)

# Vérification et correction des données
forecast_autoritaire['ds'] = pd.to_datetime(forecast_autoritaire['ds'], errors='coerce')
forecast_other['ds'] = pd.to_datetime(forecast_other['ds'], errors='coerce')

numeric_cols = ['yhat', 'yhat_lower', 'yhat_upper']
for col in numeric_cols:
    forecast_autoritaire[col] = pd.to_numeric(forecast_autoritaire[col], errors='coerce')
    forecast_other[col] = pd.to_numeric(forecast_other[col], errors='coerce')

# Suppression des lignes avec des valeurs non valides
forecast_autoritaire = forecast_autoritaire.dropna()
forecast_other = forecast_other.dropna()

# Application du style du premier graphique
plt.figure(figsize=(12, 6))
plt.plot(forecast_autoritaire['ds'], forecast_autoritaire['yhat'], label='Autoritaire', linestyle='--', color='#8F4E32')
plt.plot(forecast_other['ds'], forecast_other['yhat'], label='Autres orientations', linestyle='-', color='#CC7A56')
plt.fill_between(forecast_autoritaire['ds'], forecast_autoritaire['yhat_lower'], forecast_autoritaire['yhat_upper'], alpha=0.2, color='#8F4E32')
plt.fill_between(forecast_other['ds'], forecast_other['yhat_lower'], forecast_other['yhat_upper'], alpha=0.2, color='#CC7A56')

# Ajout des labels, titre, et légende
plt.title('Comparaison des prévisions du "Life Ladder" jusqu\'en 2027', fontsize=16, color='#8F4E32')
plt.xlabel('Année', fontsize=14, color='#8F4E32')
plt.ylabel('Life Ladder', fontsize=14, color='#8F4E32')
plt.legend(fontsize=12, loc='upper left', frameon=False)

# Définir la couleur de fond et la grille
plt.gca().set_facecolor("#FFE1C9")
plt.grid(color='white', linestyle='--', linewidth=0.5)

# Affichage du graphique
plt.show()